{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Introduction","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2023-04-10T03:51:51.057817Z","iopub.status.busy":"2023-04-10T03:51:51.057261Z","iopub.status.idle":"2023-04-10T03:51:51.067724Z","shell.execute_reply":"2023-04-10T03:51:51.066042Z","shell.execute_reply.started":"2023-04-10T03:51:51.057772Z"},"papermill":{"duration":0.013332,"end_time":"2023-04-10T05:15:31.068664","exception":false,"start_time":"2023-04-10T05:15:31.055332","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"#### Overview of the Titanic dataset\n\nThe Titanic dataset is a famous dataset in machine learning and data analysis, containing data on the passengers aboard the Titanic and their survival status. It consists of a training dataset with 891 observations and a test dataset with 418 observations. The goal is to predict the survival of passengers in the test dataset based on the variables in the training dataset.","metadata":{"papermill":{"duration":0.011944,"end_time":"2023-04-10T05:15:31.093169","exception":false,"start_time":"2023-04-10T05:15:31.081225","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"# Import libraries\nTo begin, let's import the necessary libraries that we'll be using throughout this notebook:","metadata":{"papermill":{"duration":0.012487,"end_time":"2023-04-10T05:15:31.117915","exception":false,"start_time":"2023-04-10T05:15:31.105428","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# Data Manipulation Libraries\nimport pandas as pd\nimport numpy as np\n\n# Data Visualization Libraries\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Machine Learning Libraries\nfrom sklearn.preprocessing import  StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score ,precision_score, recall_score, f1_score\nfrom sklearn.model_selection import StratifiedKFold, cross_val_score\nfrom sklearn.metrics import  classification_report, confusion_matrix\n\n# Machine Learning Models\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier,GradientBoostingClassifier, AdaBoostClassifier\nfrom xgboost import XGBClassifier\nfrom sklearn.svm import SVC","metadata":{"papermill":{"duration":1.933327,"end_time":"2023-04-10T05:15:33.065575","exception":false,"start_time":"2023-04-10T05:15:31.132248","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-09-20T08:08:34.857991Z","iopub.execute_input":"2023-09-20T08:08:34.858496Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Loading the dataset\n\nIn this section, we will load the Titanic dataset into the notebook. The dataset is stored in two CSV files, one for the training data and one for the test data. We will use the pandas library to load the CSV files into dataframes that we can manipulate and explore. Once loaded, we can begin to explore the data and prepare it for machine learning modeling","metadata":{"papermill":{"duration":0.012053,"end_time":"2023-04-10T05:15:33.090127","exception":false,"start_time":"2023-04-10T05:15:33.078074","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# reading the train data\ntrain_df = pd.read_csv(\"/kaggle/input/titanic/train.csv\")\n\n# reading the test data\ntest = pd.read_csv(\"/kaggle/input/titanic/test.csv\")","metadata":{"papermill":{"duration":0.044625,"end_time":"2023-04-10T05:15:33.147301","exception":false,"start_time":"2023-04-10T05:15:33.102676","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Understanding the Variables\n\nBefore we can begin analyzing the Titanic dataset, it's important to understand what each variable represents. The dataset contains the following variables:\n\nPassengerId: A unique identifier for each passenger.\n\nSurvived: Whether or not the passenger survived (0 = No, 1 = Yes).\n\nPclass: The passenger class (1 = 1st class, 2 = 2nd class, 3 = 3rd class).\n\nName: The name of the passenger.\n\nSex: The gender of the passenger.\n\nAge: The age of the passenger in years. Fractional values are included for infants.\n\nSibSp: The number of siblings/spouses aboard the Titanic.\n\nParch: The number of parents/children aboard the Titanic.\n\nTicket: The ticket number for the passenger.\n\nFare: The fare paid by the passenger.\n\nCabin: The cabin number for the passenger (if available).\n\nEmbarked: The port of embarkation (C = Cherbourg, Q = Queenstown, S = Southampton).","metadata":{"papermill":{"duration":0.012623,"end_time":"2023-04-10T05:15:33.173307","exception":false,"start_time":"2023-04-10T05:15:33.160684","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"# Data Exploration and Preparation","metadata":{"papermill":{"duration":0.012319,"end_time":"2023-04-10T05:15:33.198302","exception":false,"start_time":"2023-04-10T05:15:33.185983","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# we use method head() to show the first 5 rows\ntrain_df.head()\n","metadata":{"papermill":{"duration":0.046613,"end_time":"2023-04-10T05:15:33.257935","exception":false,"start_time":"2023-04-10T05:15:33.211322","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test.head()\n","metadata":{"papermill":{"duration":0.030709,"end_time":"2023-04-10T05:15:33.301481","exception":false,"start_time":"2023-04-10T05:15:33.270772","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print (test.shape)\nprint (train_df.shape)","metadata":{"papermill":{"duration":0.021639,"end_time":"2023-04-10T05:15:33.335971","exception":false,"start_time":"2023-04-10T05:15:33.314332","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"as we see above we have 11 attributes and 418 records in the test data set, but we have\n12 attributes and 891 records in the train data set.\n","metadata":{"papermill":{"duration":0.012692,"end_time":"2023-04-10T05:15:33.361774","exception":false,"start_time":"2023-04-10T05:15:33.349082","status":"completed"},"tags":[]}},{"cell_type":"code","source":"train_df.describe()","metadata":{"papermill":{"duration":0.061946,"end_time":"2023-04-10T05:15:33.436649","exception":false,"start_time":"2023-04-10T05:15:33.374703","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The above df.describe() command presents statistical properties in vertical form.\n","metadata":{"papermill":{"duration":0.012851,"end_time":"2023-04-10T05:15:33.462790","exception":false,"start_time":"2023-04-10T05:15:33.449939","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# find if we have a duplicated rows in train data\n\ntrain_df.duplicated().sum()","metadata":{"papermill":{"duration":0.030604,"end_time":"2023-04-10T05:15:33.506690","exception":false,"start_time":"2023-04-10T05:15:33.476086","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # find if we have a null values in train data\n\ntrain_df.isna().sum()","metadata":{"papermill":{"duration":0.025404,"end_time":"2023-04-10T05:15:33.545609","exception":false,"start_time":"2023-04-10T05:15:33.520205","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# find if we have a duplicated rows in test data\n\ntest.duplicated().sum()","metadata":{"papermill":{"duration":0.028493,"end_time":"2023-04-10T05:15:33.588009","exception":false,"start_time":"2023-04-10T05:15:33.559516","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# find if we have a null value in test data\ntest.isna().sum()","metadata":{"papermill":{"duration":0.026606,"end_time":"2023-04-10T05:15:33.628695","exception":false,"start_time":"2023-04-10T05:15:33.602089","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# The number of men who survived\ntrain_df[train_df['Sex']=='male']['Survived'].sum()","metadata":{"papermill":{"duration":0.026564,"end_time":"2023-04-10T05:15:33.669025","exception":false,"start_time":"2023-04-10T05:15:33.642461","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# The number of women who survived\ntrain_df[train_df['Sex']=='female']['Survived'].sum()","metadata":{"papermill":{"duration":0.025046,"end_time":"2023-04-10T05:15:33.708357","exception":false,"start_time":"2023-04-10T05:15:33.683311","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The code bellow uses the Seaborn library to plot graphs that show the number of passengers who survived and who did not survive the disaster for each of the columns 'Sex', 'Embarked', 'Pclass', 'SibSp', and 'Parch'.\n\nA loop is used to repeat the process for each specified column in the list ['Sex', 'Embarked', 'Pclass', 'SibSp', 'Parch'].\n\nThe 'countplot' function from Seaborn is used to draw the graph. This function displays the count of cases in each category for the different values of the specified column (such as gender or ticket class), along with the distinctive color for each category (survived and not survived). In other words, the count of passengers who survived and who did not survive is displayed for each different value of the specified column","metadata":{"papermill":{"duration":0.013601,"end_time":"2023-04-10T05:15:33.736657","exception":false,"start_time":"2023-04-10T05:15:33.723056","status":"completed"},"tags":[]}},{"cell_type":"code","source":"\nfor column_name in ['Sex','Embarked','Pclass', 'SibSp', 'Parch']:\n    print(column_name)\n    sns.countplot(data=train_df, x=column_name, hue='Survived')\n    plt.show()\n    print(\"\")","metadata":{"papermill":{"duration":0.865737,"end_time":"2023-04-10T05:15:34.616269","exception":false,"start_time":"2023-04-10T05:15:33.750532","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.histplot(train_df['Age'])","metadata":{"papermill":{"duration":0.23286,"end_time":"2023-04-10T05:15:34.864428","exception":false,"start_time":"2023-04-10T05:15:34.631568","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The 'histplot' function from Seaborn is used to draw the histogram. This function displays the frequency distribution of the 'Age' column, i.e., the number of passengers in each age group.\n\nThe histogram can be used to identify patterns and trends in the age distribution of the passengers in the 'train_df' dataset, such as the most common age group or the presence of outliers","metadata":{"papermill":{"duration":0.016262,"end_time":"2023-04-10T05:15:34.896728","exception":false,"start_time":"2023-04-10T05:15:34.880466","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# calculate the mean age of male passengers in the 'train_df' dataset.\nmean_male = train_df[train_df['Sex']=='male']['Age'].mean()\nmean_male","metadata":{"papermill":{"duration":0.028169,"end_time":"2023-04-10T05:15:34.940889","exception":false,"start_time":"2023-04-10T05:15:34.912720","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# calculate the mean age of بثmale passengers in the 'train_df' dataset.\nmean_female = train_df[train_df['Sex']=='female']['Age'].mean()\nmean_female","metadata":{"papermill":{"duration":0.03064,"end_time":"2023-04-10T05:15:34.988739","exception":false,"start_time":"2023-04-10T05:15:34.958099","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As we have seen, there is a difference between the mean of mal and the mean of fmale","metadata":{"papermill":{"duration":0.01598,"end_time":"2023-04-10T05:15:35.020931","exception":false,"start_time":"2023-04-10T05:15:35.004951","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"in the code bellow \nthe 'fillna' function is used to fill the missing values in the 'Age' column of male and female passengers with the value of 'mean_male' and 'mean_female' respectively.\n\nThis is done by first selecting the rows in the 'train_df' dataset where 'Sex' is 'male' using the condition 'train_df['Sex']=='male''. Then, for these selected rows, the missing values in the 'Age' column are filled with 'mean_male'.","metadata":{"papermill":{"duration":0.015818,"end_time":"2023-04-10T05:15:35.053031","exception":false,"start_time":"2023-04-10T05:15:35.037213","status":"completed"},"tags":[]}},{"cell_type":"code","source":"train_df.loc[train_df['Sex']=='male', 'Age'] = train_df[train_df['Sex']=='male']['Age'].fillna(value=mean_male)","metadata":{"papermill":{"duration":0.028027,"end_time":"2023-04-10T05:15:35.097261","exception":false,"start_time":"2023-04-10T05:15:35.069234","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df.loc[train_df['Sex']=='female', 'Age'] = train_df[train_df['Sex']=='female']['Age'].fillna(value=mean_female)","metadata":{"papermill":{"duration":0.028012,"end_time":"2023-04-10T05:15:35.142401","exception":false,"start_time":"2023-04-10T05:15:35.114389","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df.isna().sum()","metadata":{"papermill":{"duration":0.033629,"end_time":"2023-04-10T05:15:35.192957","exception":false,"start_time":"2023-04-10T05:15:35.159328","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"as we have seen, there are no null values in the column age","metadata":{"papermill":{"duration":0.016411,"end_time":"2023-04-10T05:15:35.226408","exception":false,"start_time":"2023-04-10T05:15:35.209997","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# Dropping some unimportant features\ntrain_df.drop(['PassengerId','SibSp','Parch','Ticket','Cabin','Name'], axis=1, inplace= True)","metadata":{"papermill":{"duration":0.027252,"end_time":"2023-04-10T05:15:35.269291","exception":false,"start_time":"2023-04-10T05:15:35.242039","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df.head()","metadata":{"papermill":{"duration":0.032195,"end_time":"2023-04-10T05:15:35.318456","exception":false,"start_time":"2023-04-10T05:15:35.286261","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df.dropna(inplace=True)","metadata":{"papermill":{"duration":0.036932,"end_time":"2023-04-10T05:15:35.373429","exception":false,"start_time":"2023-04-10T05:15:35.336497","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The 'dropna' function is used to remove any rows in the 'train_df' dataset that contain missing values (i.e., NaN values). The 'inplace=True' parameter is used to modify the 'train_df' dataset directly, rather than returning a new modified datasetز","metadata":{"papermill":{"duration":0.017642,"end_time":"2023-04-10T05:15:35.409645","exception":false,"start_time":"2023-04-10T05:15:35.392003","status":"completed"},"tags":[]}},{"cell_type":"code","source":"train_df.isna().sum()","metadata":{"papermill":{"duration":0.027027,"end_time":"2023-04-10T05:15:35.456339","exception":false,"start_time":"2023-04-10T05:15:35.429312","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"as we have seen above there are no null values ","metadata":{"papermill":{"duration":0.015968,"end_time":"2023-04-10T05:15:35.488663","exception":false,"start_time":"2023-04-10T05:15:35.472695","status":"completed"},"tags":[]}},{"cell_type":"code","source":"train_df.replace({'female':0,'male':1},inplace=True)","metadata":{"papermill":{"duration":0.026597,"end_time":"2023-04-10T05:15:35.531979","exception":false,"start_time":"2023-04-10T05:15:35.505382","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The 'replace' function is used to replace the values 'female' and 'male' in the 'train_df' dataset with 0 and 1, respectively. This is done by passing a dictionary with the keys 'female' and 'male' and their corresponding values 0 and 1 to the 'replace' function. The 'inplace=True' parameter is used to modify the 'train_df' dataset directly, rather than returning a new modified dataset.\n\n","metadata":{"papermill":{"duration":0.018675,"end_time":"2023-04-10T05:15:35.567675","exception":false,"start_time":"2023-04-10T05:15:35.549000","status":"completed"},"tags":[]}},{"cell_type":"code","source":"train_df = pd.get_dummies(train_df,columns=['Embarked'],prefix='Embarked')","metadata":{"papermill":{"duration":0.031998,"end_time":"2023-04-10T05:15:35.616772","exception":false,"start_time":"2023-04-10T05:15:35.584774","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The 'get_dummies' function from pandas is used to create dummy variables for the 'Embarked' column in the 'train_df' dataset. This is done by passing the 'train_df' dataset and the 'Embarked' column to the 'columns' parameter of the 'get_dummies' function. The 'prefix' parameter is used to add the prefix 'Embarked_' to the column names of the resulting dummy variables.\n\nThe resulting 'train_df' dataset now contains new columns 'Embarked_C', 'Embarked_Q', and 'Embarked_S', with binary values (0 or 1) indicating whether a passenger embarked from the corresponding port.","metadata":{"papermill":{"duration":0.016274,"end_time":"2023-04-10T05:15:35.649618","exception":false,"start_time":"2023-04-10T05:15:35.633344","status":"completed"},"tags":[]}},{"cell_type":"code","source":"train_df.head()","metadata":{"papermill":{"duration":0.033464,"end_time":"2023-04-10T05:15:35.699445","exception":false,"start_time":"2023-04-10T05:15:35.665981","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### split train data","metadata":{"papermill":{"duration":0.017574,"end_time":"2023-04-10T05:15:35.733988","exception":false,"start_time":"2023-04-10T05:15:35.716414","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# split data into x and y\nx = train_df.drop('Survived',axis =1)\ny = train_df['Survived']\nx","metadata":{"papermill":{"duration":0.035875,"end_time":"2023-04-10T05:15:35.786757","exception":false,"start_time":"2023-04-10T05:15:35.750882","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This code standardizes the values of the 'Age' and 'Fare' columns in the 'x' dataframe using the StandardScaler from the scikit-learn library.","metadata":{"papermill":{"duration":0.016622,"end_time":"2023-04-10T05:15:35.820747","exception":false,"start_time":"2023-04-10T05:15:35.804125","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# Select numerical columns\nnum_cols = ['Age', 'Fare']\n\n# Create scaler object\nscaler = StandardScaler()\n\n# Fit scaler on selected columns\nscaler.fit(x[num_cols])\n\n# Transform selected columns with scaler\nx[num_cols] = scaler.transform(x[num_cols])","metadata":{"papermill":{"duration":0.030976,"end_time":"2023-04-10T05:15:35.868405","exception":false,"start_time":"2023-04-10T05:15:35.837429","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# split train data into train and test\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=10)","metadata":{"papermill":{"duration":0.03042,"end_time":"2023-04-10T05:15:35.915940","exception":false,"start_time":"2023-04-10T05:15:35.885520","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# modeling","metadata":{"papermill":{"duration":0.017124,"end_time":"2023-04-10T05:15:35.950248","exception":false,"start_time":"2023-04-10T05:15:35.933124","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# Initialize the models\nmodels = {\n    'Logistic Regression': LogisticRegression(),\n    'Decision Tree': DecisionTreeClassifier(),\n    'Random Forest': RandomForestClassifier(),\n    'SVM': SVC(),\n    'XGBClassifier':XGBClassifier(),\n    'GradientBoostingClassifier':GradientBoostingClassifier(),\n    'AdaBoostClassifier':AdaBoostClassifier()\n    \n}\n\n# Train and evaluate each model using cross-validation\nfor name, model in models.items():\n    scores = cross_val_score(model, x_train, y_train, cv=5, scoring='accuracy')\n    print(f\"{name} accuracy: {scores.mean():.3f} +/- {scores.std():.3f}\")\n    \n    # Fit the model to the full training set and make predictions on the test set\n    model.fit(x_train, y_train)\n    y_pred = model.predict(x_test)\n    \n    # Evaluate the model on the test set\n    acc = accuracy_score(y_test, y_pred)\n    prec = precision_score(y_test, y_pred)\n    rec = recall_score(y_test, y_pred)\n    f1 = f1_score(y_test, y_pred)\n    \n    print(f\"Accuracy: {acc:.3f}\")\n    print(f\"Precision: {prec:.3f}\")\n    print(f\"Recall: {rec:.3f}\")\n    print(f\"F1-score: {f1:.3f}\")\n    print()","metadata":{"papermill":{"duration":3.179313,"end_time":"2023-04-10T05:15:39.146880","exception":false,"start_time":"2023-04-10T05:15:35.967567","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"After comparing the accuracy of several models, we found that the GradientBoostingClassifier model had the highest accuracy.\n\nSo I will use it to make a prediction to the test set.","metadata":{"papermill":{"duration":0.017955,"end_time":"2023-04-10T05:15:39.182692","exception":false,"start_time":"2023-04-10T05:15:39.164737","status":"completed"},"tags":[]}},{"cell_type":"code","source":"gbc= GradientBoostingClassifier()\nscores = cross_val_score(gbc, x_train, y_train, cv=5, scoring='accuracy')\nprint(f\"{gbc} accuracy: {scores}\")\n    \n# Fit the model to the full training set and make predictions on the test set\ngbc.fit(x_train, y_train)\ny_pred = gbc.predict(x_test)\n# Evaluate the model on the test set\nacc = accuracy_score(y_test, y_pred)\nprint (acc)","metadata":{"papermill":{"duration":0.489857,"end_time":"2023-04-10T05:15:39.691052","exception":false,"start_time":"2023-04-10T05:15:39.201195","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Test data preparation","metadata":{"papermill":{"duration":0.017191,"end_time":"2023-04-10T05:15:39.725978","exception":false,"start_time":"2023-04-10T05:15:39.708787","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# show the first 5 columns \ntest.head()","metadata":{"papermill":{"duration":0.03786,"end_time":"2023-04-10T05:15:39.781651","exception":false,"start_time":"2023-04-10T05:15:39.743791","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# fill the null value that in the age column\nmean_male_test = test[test['Sex']=='male']['Age'].mean()\nmean_female_test = test[test['Sex']=='female']['Age'].mean()\ntest.loc[test['Sex']=='male', 'Age'] = test[test['Sex']=='male']['Age'].fillna(value=mean_male_test)\ntest.loc[test['Sex']=='female', 'Age'] = test[test['Sex']=='female']['Age'].fillna(value=mean_female_test)\n\n# fill the null values that in the Fare column\ntest['Fare'].fillna(test['Fare'].median(), inplace = True)\n\n# Dropping some unimportant features\ntest.drop(['SibSp','Parch','Ticket','Cabin','Name'], axis=1, inplace= True)\n\n\n","metadata":{"papermill":{"duration":0.035507,"end_time":"2023-04-10T05:15:39.835749","exception":false,"start_time":"2023-04-10T05:15:39.800242","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test.isna().sum()","metadata":{"papermill":{"duration":0.031446,"end_time":"2023-04-10T05:15:39.884234","exception":false,"start_time":"2023-04-10T05:15:39.852788","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"as we have seen above, there are no null values","metadata":{"papermill":{"duration":0.017798,"end_time":"2023-04-10T05:15:39.920078","exception":false,"start_time":"2023-04-10T05:15:39.902280","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# convert string values to numeric by replace the female and male by 0 and 1\ntest.replace({'female':0,'male':1},inplace=True)","metadata":{"papermill":{"duration":0.030756,"end_time":"2023-04-10T05:15:39.968560","exception":false,"start_time":"2023-04-10T05:15:39.937804","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# using one hot encoder to convertr str values \ntest = pd.get_dummies(test,columns=['Embarked'],prefix='Embarked')","metadata":{"papermill":{"duration":0.033491,"end_time":"2023-04-10T05:15:40.020500","exception":false,"start_time":"2023-04-10T05:15:39.987009","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Fit scaler on numeric columns\nscaler.fit(test[num_cols])\n\n# Transform numeric columns with scaler\ntest[num_cols] = scaler.transform(test[num_cols])","metadata":{"papermill":{"duration":0.0344,"end_time":"2023-04-10T05:15:40.073177","exception":false,"start_time":"2023-04-10T05:15:40.038777","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test.head()","metadata":{"papermill":{"duration":0.033945,"end_time":"2023-04-10T05:15:40.125845","exception":false,"start_time":"2023-04-10T05:15:40.091900","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Store the PassengerId column in a separate variable\n\nPassengerId = test['PassengerId']\n\n# drop PassengerId column from the test set\ntest.drop('PassengerId',axis=1,inplace=True)","metadata":{"papermill":{"duration":0.028645,"end_time":"2023-04-10T05:15:40.171971","exception":false,"start_time":"2023-04-10T05:15:40.143326","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Predictions and Submission","metadata":{"papermill":{"duration":0.019944,"end_time":"2023-04-10T05:15:40.209965","exception":false,"start_time":"2023-04-10T05:15:40.190021","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# #Generate predictions for the test data using GradientBoostingClassifier \ntest_pred = gbc.predict(test)","metadata":{"papermill":{"duration":0.03103,"end_time":"2023-04-10T05:15:40.259509","exception":false,"start_time":"2023-04-10T05:15:40.228479","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\nCreate a submission file with the PassengerId column and predicted survival outcomes\n","metadata":{"papermill":{"duration":0.017475,"end_time":"2023-04-10T05:15:40.295098","exception":false,"start_time":"2023-04-10T05:15:40.277623","status":"completed"},"tags":[]}},{"cell_type":"code","source":"submission = pd.DataFrame({'PassengerId': PassengerId, 'Survived': test_pred})","metadata":{"papermill":{"duration":0.027477,"end_time":"2023-04-10T05:15:40.341350","exception":false,"start_time":"2023-04-10T05:15:40.313873","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission.to_csv('submission.csv', index=False)","metadata":{"papermill":{"duration":0.032179,"end_time":"2023-04-10T05:15:40.391292","exception":false,"start_time":"2023-04-10T05:15:40.359113","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"papermill":{"duration":0.017476,"end_time":"2023-04-10T05:15:40.426684","exception":false,"start_time":"2023-04-10T05:15:40.409208","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]}]}